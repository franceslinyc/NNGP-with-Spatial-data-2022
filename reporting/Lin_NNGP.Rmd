---
title: "NNGP Models with Spatial Examples of Simulated Data and... "
author: "Frances Lin"
date: "Dec 2022"
output: pdf_document
header-includes: \usepackage{setspace}\onehalfspacing
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Introduction 

Inferences and predictions on large spatial data or data with locations $\approx 10^6$ have either been too computationally challenging or infeasible. Methods for large spatial data that are under active development. However, most of the existing methods has focused primarily on theoretical and methodological developments. These methods have paid little attention on the algorithmic details and have not made use of high-performance computing (HPC) libraries to expedite expensive computations and delivery full Bayesian inference for large spatial data. 

On the other hand, while the original NNGP (nearest neighbor Gaussian process) model (Datta et al., 2016) appears promising, this model is prone to high autocorrelations and slow convergence because the "sequential" Gibbs sampler involves updating a high-dimensional vector of latent random effect. The original NNPG model by Datta et al. (2016) is refereed to as the latent NNGP model. 

Three alternate formulations of the NNGP model that are more efficient and practical than the latent NNGP model (2016) are proposed: (1) the collapsed NNGP model, (2) the response NNGP model, and (3) the conjugate NNGP model, and these models are accessible through the `R` package `spNNGP` (Finley et al., 2021). 

Section 2 reviews GPs (Gaussian processes) and introduces the latent NNGP model, which is followed by three alternate formulations of the latent NNGP model: (1) a collapsed NNGP model, (2) a NNGP model for the response (with no latent process), and (3) a conjugate NNGP model that allows for MCMC-free inference. Section 3. ... Section 4 includes the discussion. ... are included in the Appendix. 

# 2. Nearest Neighbor Gaussian Processes

## Review of GPs for spatial data

A spatial linear mixed effects model is given as 
$$
y(s_i) = {x(s_i)}^T \beta + w(s_i) + \epsilon(s_i), 
$$
where $y(s_i)$ denotes the response, $x(s_i)$ denotes the known or observed covariates, $\beta$ is the vector of coefficients, $w(s_i)$ is the vector of unknown or unobserved covariates, and $\epsilon(s_i) \sim^{iid} N(0, \tau^2)$ is the random noise. 

Gaussian processes (GPs) are widely used in machine learning to model smooth functions for regression, classification, and other tasks (Rasmussen 2003, as cited in Finley et al., 2021). In spatial statistics, GPs are commonly used to model the latent surface $w(s)$. A GP model for the spatial surface 
$$
w(s) \sim GP(0, C(\cdot, \cdot | \theta)), 
$$
where $C(\cdot, \cdot | \theta)$ is a covariance function, implies that $w = {(w(s_1),..., w(s_n))}^T$ follows a multivariate Gaussian distribution with mean zero and covariance matrix $C(\theta) = C = (c_{ij})$, where $c_{ij} = C(s_i, s_j | \theta)$ and $\theta$ is the covariance parameters of the GP. 

A popular choice of $\theta$ for $C(\cdot, \cdot | \theta)$ is selected from the Matérn covariance function. Let $s_i$ and $s_j$ be two points in $\mathcal{D}$, then the Matérn covariance function is specified as 
$$
C(s_i, s_j; \sigma^2, \phi, \nu) = \frac{\sigma^2} {2^{\nu - 1} \Gamma(\nu)} {(||s_i - s_j|| \phi)}^{\nu} \mathcal{K}_\nu {(||s_i - s_j|| \phi)}, \ \ \ \ \phi > 0, \nu > 0, 
$$
where $\theta = \{ \sigma^2, \phi, \nu \}$ are respectively the marginal variance, scale (inverse of range) and smoothness parameters, $\Gamma$ is the gamma function, $||\cdot||$ denotes the Euclidean distance in $\mathbb{R}^d$, and $\mathcal{K}$ is the Bessel function of second kind. 

Customary Bayesian hierarchical models are constructed as 
$$
p(\beta, \theta, \tau^2) \times N(w | 0, C(\theta)) \times N(y | X \beta + w, \tau^2 I), \tag{2}
$$
where $p(\beta, \theta, \tau^2)$ is specified by assigning priors to $\beta, \theta$ and $\tau^2$. 

Alternatively, the marginal model can be constricted by integrating $w$ out from (2) and is given as 
$$
N(y | X \beta, C(\theta) + \tau^2 I).
$$

...... (Finley et al., 2021). 


## NNGPs for spatial data 

When $n$ is large, evaluating (2) can be computationally challenging or infeasible. More specifically, evaluating $N(w | 0, C)$ requires $\mathcal{O}(n^3)$ computations and storing the matrix $C$ requires $\mathcal{O}(n^2)$ storage. In addition, predicting the response at new locations $K$ requires additional $\mathcal{O}(kn^2)$ operations. Unfortunately, integrating $w$ out from (2) does not always give computational advantages either. 

The underlying idea of the NNGP models is similar to that of the graphical models. More specifically, the joint distribution for a random vector $w$ can be viewed as a directed acyclic graph (DAG). That is, $p(w) = p(w_1, w_2, ... w_n)$ can be written as 
$$
p(w) = p(w_1) \prod_{i = 2}^n p(w_i | Pa[i]), \tag{A1}
$$
where $w_i \equiv w(s_i)$ and $Pa[i] = \{w_1, w_2,... w_{i-1}\}$ is a set of parents of $w_i$, 

or, more explicitly, as 
$$
p(w) = p(w_1) p(w_2 |w_1) p(w_3 |w_1, w_2) \cdots p(w_n |w_1, w_2,... w_{i-1}) \tag{A2} 
$$
(Datta et al., 2017). Sparse models for $w$ can be constructed by shrinking the size of $Pa[i]$. ......

...... The multivariate Gaussian density $N(w | 0, C)$ (or $w \sim N(0, C(\theta))$) in (2) can be written as a linear model 
$$
w_1 = 0 + \eta_1, 
$$
$$
w_2 = a_{21}w_1 + \eta_2, 
$$
$$
w_i = a_{i1}w_1 + a_{i2}w_2 + \cdots + a_{i,i-1}w_{i-1} + \eta_i, \ \text{for} \ i = 2,...,n, \tag{A3}
$$

or, more explicitly, as 
$$
\begin{bmatrix}
w_1 \\
w_2 \\
w_3 \\
\vdots \\
w_n
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 & \cdots & 0 & 0 \\
a_{21} & 0 & \cdots & 0 & 0 \\
a_{31} & a_{32} & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn_1} & 0 \\
\end{bmatrix} + 
\begin{bmatrix}
\eta_1 \\
\eta_2 \\
\eta_3 \\
\vdots \\
\eta_n \\
\end{bmatrix} \tag{A4}
$$

(Datta et al., 2017), 

or, more compactly, as 
$$
w = A w + \eta, \tag{A5}
$$
where $A$ is $nxn$ strictly lower-triangular and $\eta \sim N(0, D)$ with $D = (d_1, d_2,..., d_n)$ is diagonal. It follows that $I - A$ is nonsingular and, by the Cholesky factorization (Cholesky decomposition), a covariance matrix $C$ can be factorized into a product $C = {(I - A)}^{-1} D {(I - A)}^{-T}$, where for any matrix $M$, ${M}^{-T}$ refers to the inverse of its transpose. 

However, the Cholesky factorization for the full GP covariance $C$ does not offer any computational advantages. Instead, the sparsity was introduced through graphical models (Datta et al., 2017). 

To construct a sparse precision matrix, start with a dense $nxn$ covariance matrix $C$ and construct a sparse strictly lower-triangular matrix $A$ with no more than $m(\ll n)$ nonzero entries in each row and the diagonal matrix $D$, then the matrix $\tilde{C} = {(I - A)}^{-1} D {(I - A)}^{-T}$ is a covariance matrix and its inverse ${\tilde{C}}^{-1} = {(I - A)}^{T} D^{-1} {(I - A)}^{}$ is sparse. This leads to the latent NNGP model in the section below. 

NNGP can also be viewed as (Finley et al., 2021).

## 2.0. Latent NNGP 

The original NNGP model proposed by Datta et al. (2016) constructed the neighbor sets based on $m$ nearest neighbors and replaced the GP prior for spatial random effects $w$ in (2) with a NNGP prior 
$$
w \sim N(0, \tilde{C}(\theta)).
$$
This model is referred to as the latent NNGP, which uses a fully Bayesian hierarchical specification 
$$
p(\beta, \theta, \tau^2) \times N(w | 0, \tilde{C}(\theta)) \times N(y | X \beta + w, \tau^2 I), \tag{2*}
$$
for running an MCMC (Markov chain Monte Carlo) algorithm (Finley et al., 2021). 

Normal priors for $\beta$ and inverse Gamma priors for the variance components $\tau^2$ ensure that they yield conjugate full conditionals in the Gibbs sampler (Finley et al., 2021). The remaining covariance parameters $\theta$ are updated using random-walk Metropolis steps for their respective full conditionals (Finley et al., 2021). 

The full conditional distribution for $w$ in (2*) is...... However, this block update of $w$ is not practical (Finley et al., 2021). 

The MCMC implementation of the latent NNGP involves updating the $n$ latent spatial effects $w$ sequentially. While......, the high-dimensional MCMC model......


## 2.1. Collapsed NNGP

The collapsed NNGP model...... 

A collapsed NNGP model not only enjoys the frugality of a low-dimensional MCMC chain but also allows for full recovery of the latent random effects $w$. 

Consider the two-stage hierarchical specification 
$$
N(w | 0, \tilde{C}) \times N(y | X \beta + w, \tau^2 I)
$$
and integrate out $w$ to avoid sampling $w$ in the Gibb's sampler, then the collapsed NNGP model is specified as 
$$
y \sim N(X \beta, \Lambda = \tilde{C} + \tau^2 I),  \tag{3}
$$
where $\tilde{C} = \tilde{C}(\theta)$ and $\theta = \{ \sigma^2, \phi, \nu \}$ for Matérn covariance function. 

A normal prior $N(\mu_{\beta}, V_{\beta})$ is used for $\beta$, inverse-Gamma priors are used for the spatial and noise variances $\sigma^2$ and $\tau^2$, and uniform priors are used for the range and smoothness parameters $1/\phi$ and $\nu$. 

Explain Algorithm 1 in words here.


## 2.2. NNGP for the Response

Both the latent NNGP (sequential NNGP) and the collapsed version of it the collapsed NNGP make predication at a new location by first recovering the spatial random effects $w$ and predicting value at the new location with kriging. However, if inference on the latent process is of interest, the recovery of $w$ is necessary. Otherwise, it is often a computational burden. ......

The response NNGP model...... 

Consider the marginal Gaussian process for the response
$$
\{y(s)\} \sim GP({x(s)}^T \beta, \Sigma(\cdot, \cdot)), 
$$
where $\Sigma$ is the marginalized covariance function $\Sigma$ and is specified as $\Sigma(s_i, s_j) = C(s_i, s_j | \theta) + \tau^2 \delta(s_i, s_j),$ where $\delta$ is the Kronecker delta (Finley et al., 2021). 

Since the covariance function of an NNGP can be derived from any parent GP, next replace the full GP covariance $\Sigma$ with its NNGP analogue $\tilde{\Sigma}$, then the response NNGP marginal model is specified as 
$$
Y \sim N(X \beta, \tilde{\Sigma}), \tag{4}
$$
where $\tilde{\Sigma}$ is the NNGP covariance matrix derived from $\Sigma = C(\theta) + \tau^2 I.$ (Finley et al., 2021). The sparsity in Section 2 can be applied to $\tilde{\Sigma}^{-1}$. 


## 2.3. MCMC-Free Exact Bayesian Inference Using Conjugate NNGP 

...... MCMC methods are used to obtain approximate inference. ...... However, running MCMC methods such as the Gibbs’ sampler for several thousand iterations may still be very slow. 

The conjugate NNGP model...... 

Consider the marginal model from Section 2.2 
$$
Y \sim N(X \beta, {\Sigma})
$$
and define $\alpha$ as $\alpha = \tau^2 / \sigma^2$, then the MCMC-free marginal model can be rewritten as 
$$
Y \sim N(X \beta, \sigma^2 {M}), \tag{5}
$$
where ${M} = G + \alpha I$ and $G$ is the Matérn correlation matrix corresponding to the covariance matrix $C$, that is, $G[i,j] = C(s_i, s_j, {(1, \nu, \phi)}^T)$.

Replace $M$ with $\tilde{M}$, then the analogous NNGP model is specified as 
$$
Y \sim N(X \beta, \sigma^2 \tilde{M}),
$$
where $\tilde{M} = \tilde{M}(\alpha, \phi)$ depends on $\alpha$, the spatial range $\phi$, and smoothness parameter $\nu$. 


# 3. Applications to Spatial data

## 3.1. Simulated data 

## 3.2. Real data 

# 4. 


\newpage 

# Reference 

Finley, A. O., Datta, A., Cook, B. D., Morton, D. C., Andersen, H. E., & Banerjee, S. (2019). Efficient algorithms for Bayesian nearest neighbor Gaussian processes. Journal of Computational and Graphical Statistics, 28(2), 401-414.

